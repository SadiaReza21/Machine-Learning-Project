# -*- coding: utf-8 -*-
"""CSE445 Project 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y9LSxQFdSAiALbQZDM3iXiuKI6dcIddk
"""

from google.colab import files  #upload the cleaned dataset
uploaded = files.upload()

df = pd.read_csv("cleaned_mental_health_dataset.csv")

import pandas as pd
from sklearn.preprocessing import LabelEncoder

df = pd.read_csv("cleaned_mental_health_dataset.csv")
df.head()  # Shows first 5 rows

label_columns = [
    'Mental_Health_Condition',
    'Severity',
    'Consultation_History',
    'Stress_Level',
    'Diet_Quality',
    'Smoking_Habit',
    'Alcohol_Consumption',
    'Medication_Usage'
]

le = LabelEncoder()
for col in label_columns:
    df[col] = le.fit_transform(df[col])

one_hot_columns = ['Gender', 'Occupation', 'Country']

df = pd.get_dummies(df, columns=one_hot_columns)

df.head()

df.to_csv("encoded_mental_health_dataset.csv", index=False)  #download the cleaned and encoded dataset
files.download("encoded_mental_health_dataset.csv")

import pandas as pd                                                            #import Decision Tree model
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

from google.colab import files  #import the cleaned, encoded, and scaled dataset i.e. final dataset for model training
uploaded = files.upload()

df = pd.read_csv("final_mental_health_data.csv")
df.head()  # Shows first 5 rows

X = df.drop('Severity', axis=1)  #Target variable is 'Severity'
y = df['Severity']

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    stratify=y,
    random_state=42
)

dt_model = DecisionTreeClassifier(random_state=42)  #I selected standard random state = 42 for consistency
dt_model.fit(X_train, y_train)

scores = cross_val_score(dt_model, X_train, y_train, cv=5, scoring='accuracy')    #checking cross validation accuracy
print("Cross-validation scores for each fold:", scores)
print("Average cross-validation accuracy:", scores.mean())

dt_tuned = DecisionTreeClassifier(        #fine tuning hyperparameters to increase cross validation accuracy
    max_depth=5,
    min_samples_split=4,
    criterion='gini',
    random_state=42
)
tuned_scores = cross_val_score(dt_tuned, X_train, y_train, cv=5, scoring='accuracy')
print("Tuned cross-validation accuracy:", tuned_scores.mean())

from sklearn.model_selection import GridSearchCV     #finding optimum hyperparameters for optimum cross validation accuracy using GridSearchCV function from sklearn library
from sklearn.tree import DecisionTreeClassifier

# Create base model
dt = DecisionTreeClassifier(random_state=42)

# Hyperparameter grid
param_grid = {
    'max_depth': [3, 5, 8, 12, None],
    'min_samples_split': [2, 4, 6, 10],
    'min_samples_leaf': [1, 2, 4, 6],
    'criterion': ['gini', 'entropy'],
    'max_features': [None, 'sqrt', 'log2']
}

# Grid search
grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

print("Best Parameters:", grid_search.best_params_)
print("Best Cross-Validation Accuracy:", grid_search.best_score_)

dt_tuned = DecisionTreeClassifier(    #fine tuning by using optimum hyperparameters
    max_depth=None,
    min_samples_split=6,
    criterion='entropy',
    random_state=42,
    max_features='log2',
    min_samples_leaf=1
)
tuned_scores = cross_val_score(dt_tuned, X_train, y_train, cv=5, scoring='accuracy')
print("Tuned cross-validation accuracy:", tuned_scores.mean())

importances = pd.DataFrame({            #finding most important features to the target variable using feature_importances function
    'Feature': X.columns,
    'Importance': rf_model.feature_importances_
}).sort_values(by='Importance', ascending=False)

print(importances.head(15))

df = pd.read_csv("decision_tree_final_mental_health_dataset.csv")    #dataset for decision tree after removing noise to increase cross-validation accuracy
df.head()

X = df.drop('Severity', axis=1)
y = df['Severity']

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    stratify=y,
    random_state=42
)

dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)

scores = cross_val_score(dt_model, X_train, y_train, cv=5, scoring='accuracy')
print("Cross-validation scores for each fold:", scores)
print("Average cross-validation accuracy:", scores.mean())

dt_tuned = DecisionTreeClassifier(
    max_depth=5,             # Limit depth to avoid overfitting
    min_samples_split=4,     # Minimum samples to split a node
    criterion='gini',        # Try 'entropy' as well
    random_state=42
)
tuned_scores = cross_val_score(dt_tuned, X_train, y_train, cv=5, scoring='accuracy')
print("Tuned CV Accuracy:", tuned_scores.mean())

from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier

# Create base model
dt = DecisionTreeClassifier(random_state=42)

# Hyperparameter grid
param_grid = {
    'max_depth': [3, 5, 8, 12, None],
    'min_samples_split': [2, 4, 6, 10],
    'min_samples_leaf': [1, 2, 4, 6],
    'criterion': ['gini', 'entropy'],
    'max_features': [None, 'sqrt', 'log2']
}

# Grid search
grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

print("Best Parameters:", grid_search.best_params_)
print("Best Cross-Validation Accuracy:", grid_search.best_score_)

dt_tuned = DecisionTreeClassifier(
    max_depth=8,
    min_samples_split=2,
    criterion='gini',
    random_state=42,
    max_features='sqrt',
    min_samples_leaf=2
)
tuned_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='accuracy')
print("Cross-validation scores:", tuned_scores)
print("Tuned CV accuracy:", tuned_scores.mean())

y_pred = rf_model.predict(X_test)          #Test accuracy

print("Test Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))